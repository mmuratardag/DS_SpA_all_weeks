{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "So far we have grouped Machine Learning into Supervised Learning and\n",
    "Unsupervised Learning. There is a third branch of Machine Learning called\n",
    "Reinforcement Learning. It is motivated by the way humans are belived to learn,\n",
    "by **interacting with their environment**.\n",
    "\n",
    "The goal of Reinforcement Learning is to map actions to situations (states) so as to \n",
    "**maximize a numerical reward signal**.\n",
    "\n",
    "https://www.youtube.com/watch?v=kopoLzvh5jY\n",
    "\n",
    "## How does it work?\n",
    "\n",
    "We define an **agent**, that takes **actions**. These actions lead to a **reward** and influence the **state** of the environment.\n",
    "\n",
    "![title](SAR.jpg)\n",
    "\n",
    "## Problem Definition\n",
    "\n",
    "Reinforcement Learning problems are commonly defined as Markov Decision Processes (MDP) that are defined by a **state space** *S*, an **action space** *A*, a **state transition function** *P*, a **reward function** *r* and a **discount factor** *$\\gamma$*.\n",
    "\n",
    "![](mdp.png)\n",
    "\n",
    "### State Space *S*\n",
    "\n",
    "$S = \\{s_1, s_2, ..., s_n\\}$\n",
    "\n",
    "All n possible states of the environment.\n",
    "\n",
    "### Action Space *A*\n",
    "\n",
    "$A = \\{a_1, a_2, ..., a_m\\}$\n",
    "\n",
    "All m possible actions of the agent.\n",
    "\n",
    "### State Transition Function *P*\n",
    "\n",
    "$P(s', r|s, a)$\n",
    "\n",
    "The probability distribution of entering state $s'$ and receiving reward *r* after choosing action *a* in state *s*. It defines the dynamics of the MDP.\n",
    "\n",
    "### Reward Function\n",
    "\n",
    "$R(a, s)$\n",
    "\n",
    "The reward of the agent for choosing action a in state s.\n",
    "\n",
    "### Discount Factor\n",
    "\n",
    "$\\gamma$\n",
    "\n",
    "The factor with which future rewards are discounted. Usually a discount factor $\\gamma < 1$ is used to indicate that future reward is worth less than current reward.\n",
    "\n",
    "## Goal\n",
    "\n",
    "Find a policy $\\pi(a|s)$ that maximizes the total expected reward $V_\\pi(s) = E[G_t|S_t = s] \\forall s$ where $G_t = R_{t+1} + \\gamma * R_{t+2} ... + \\gamma^{p-1} * R_{t+p}$ is the **return**.\n",
    "\n",
    "#### Return\n",
    "\n",
    "$G_t = R_{t+1} + \\gamma * R_{t+2} ... + \\gamma^{p-1} * R_{t+p}$\n",
    "\n",
    "The return at time t is the cumulated future discounted return.\n",
    "\n",
    "#### Policy\n",
    "\n",
    "$\\pi(a|s)$\n",
    "\n",
    "Is a mapping of a single action to every state of the environment.\n",
    "\n",
    "* How probable is it for an agent to select any action from a given state? **Policies**\n",
    "* How good is any given action or any given state for an agent? **Value functions**\n",
    "\n",
    "**Value functions**\n",
    "\n",
    "#### State-Value Function\n",
    "\n",
    "$V_\\pi(s) = E[G_t|S_t=s]$\n",
    "\n",
    "The state-value function for policy $\\pi(a|s)$ is the total expected return from being in state S=s and following the policy $\\pi(a|s)$.\n",
    "\n",
    "#### Action-Value Function\n",
    "\n",
    "$Q_\\pi(s, a) = E[G_t|S_t=s, A_t=a]$\n",
    "\n",
    "The action-value function for policy $\\pi(a|s)$ is the total expected return from being in state S=s, chosing action A=a and thereafter following policy $\\pi(a|s)$.\n",
    "\n",
    "One important property of both value functions is that they are **recursive relationships**. E.g.\n",
    "\n",
    "\n",
    "$Q_\\pi(s, a) = E[G_t|S_t=s, A_t=a] = $\n",
    "\n",
    "$E[R_{t+1} + \\gamma*G_{t+1}|S_t=s, A_t=a] = $\n",
    "\n",
    "$E[R_{t+1}|S_t=s, A_t=a] + E[\\gamma*V_\\pi(s')|S_t+1=s']$\n",
    "\n",
    "This is called the **Bellman equation** and is central to Reinforcement Learning.\n",
    "\n",
    "## Summary\n",
    "\n",
    "We want to find a policy that optimizes the return (sum of discounted future rewards) of the agent.\n",
    "\n",
    "Let's look at an example.\n",
    "\n",
    "Assumptions:\n",
    "\n",
    "- Mouse can go left, right, up and down\n",
    "- If the mouse finds cheese, either in the upper left corner or in the lower right corner, the environment terminates\n",
    "- The mouse prefers two blocks of cheese over one block of cheese\n",
    "\n",
    "![](MOUSE_GRID.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION:\n",
    "So what are state space, action space, state transition function, reward function and discount factor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we find the policy?\n",
    "\n",
    "1. Dynamic Programming: Do not learn from the environment and require knowledge of the dynamics and the rewards\n",
    "- Monte Carlo Methods: Learn directly from the environment but only after the final outcome is observed\n",
    "- Temporal Difference Learning: Learn directly from the environment and update estimates from other learned estimates\n",
    "- Policy Gradients: Approximate the value function through a parametrized function (Deep Reinforcement Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawbacks\n",
    "\n",
    "* Credit assignment Problem\n",
    "* Exploration vs. Exploitation\n",
    "* Sparse reward symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement it in practice using OpenAI's Gym\n",
    "* A handy library for learning about RL - https://gym.openai.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install gym`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's work on the cartpole problem\n",
    "#### First we make an environment in which the agent can be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We survived 35 steps\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for i in range(1000):\n",
    "    env.render()\n",
    "    obs, reward, done, _ = env.step(env.action_space.sample()) # take a random action\n",
    "    time.sleep(0.08)\n",
    "    if done:\n",
    "        print(f'We survived {i} steps')\n",
    "        env.reset()\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we implement the agent-environment loop\n",
    "* Start the process by resetting the environment\n",
    "* And return an initial observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0264197 , -0.02788114,  0.00047485, -0.00742791])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\[position of cart, velocity of cart, angle of pole, rotation rate of pole\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can achieve the same thing by taking an action - in this case a  `step` in a given direction, 0 for left and 1 for right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.step(0) # move cart left \n",
    "obs, reward, done, _ = env.step(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already use the `done` boolean to work out if we can stop the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.03143752, -0.02789261,  0.00603439, -0.0071752 ]), 1.0, False, {})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, reward, done, _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use `sample` the `action_space` space to randomly pick an action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_step = env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And `render` the environment to see what our cart is doing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OK, but we need to build an RL agent. What next?**\n",
    "\n",
    "First, lets try to build the simplest RL agent:\n",
    "* If the pole is left, move left\n",
    "* If the pole is right, move right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_rl(env):\n",
    "    #reset the environment and taking an initial step\n",
    "    obs = env.reset()\n",
    "    \n",
    "     #loop over this process until I die\n",
    "    for i in range(1000):\n",
    "        \n",
    "    #measure: is my pole angled to the left, or the right\n",
    "    #action: if pole is left, move cart left. if pole is right, move right\n",
    "        if obs[2] < 0:\n",
    "            action = 0\n",
    "        elif obs[2] > 0:\n",
    "            action = 1\n",
    "        elif obs[2] == 0:\n",
    "            print('omgomgomg were amazing')\n",
    "            break\n",
    "            \n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        env.render()\n",
    "        time.sleep(0.08) #to make the video play at a normal rate\n",
    "        if done:\n",
    "            print(f'iterations survived: {i}')\n",
    "            env.close()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#benchmark for a dumb rl agent = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations survived: 37\n"
     ]
    }
   ],
   "source": [
    "simple_rl(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at some evolutionary algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.50146414, -0.39732256,  0.69022968, -0.54436766])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(4) *2 -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = np.random.rand(4) * 2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.97265271, -0.2421153 , -0.16512412, -0.95989455])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01889151, -0.02661885,  0.01797036, -0.01824947])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002620181811704621"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(parameters, observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = 0 if np.matmul(parameters,observation) < 0 else 1\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, parameters, range_=10000, render=False):  \n",
    "    observation = env.reset()\n",
    "    totalreward = 0\n",
    "    \n",
    "    for _ in range(range_):\n",
    "        action = 0 if np.matmul(parameters,observation) < 0 else 1\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        totalreward += reward\n",
    "        if render:\n",
    "            env.render()\n",
    "            time.sleep(0.08)\n",
    "        if done:\n",
    "            print('yop')\n",
    "            break\n",
    "            \n",
    "    env.close()\n",
    "    return totalreward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yop\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_episode(env, parameters, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yop\n",
      "yop\n",
      "yop\n",
      "yop\n",
      "yop\n",
      "yop\n",
      "yop\n",
      "yop\n",
      "yop\n",
      "yop\n",
      "yop\n",
      "yop\n",
      "yop\n",
      "yop\n",
      "yop\n",
      "yop\n",
      "yop\n",
      "yop\n",
      "yop\n",
      "yop\n",
      "yop\n",
      "yop\n",
      "yop\n",
      "yop\n",
      "yop\n",
      "yop\n",
      "yop\n",
      "yop\n",
      "yop\n",
      "yop\n",
      "yop\n",
      "yop\n",
      "yop\n",
      "yop\n",
      "yop\n",
      "yop\n",
      "35 episodes required to reach a reward of 500\n"
     ]
    }
   ],
   "source": [
    "max_timesteps = 500\n",
    "bestparams = None  \n",
    "bestreward = 0  \n",
    "all_rewards = []\n",
    "for i in range(1000):  \n",
    "    parameters = np.random.rand(4) * 2 - 1\n",
    "    reward = run_episode(env,parameters, range_=max_timesteps)\n",
    "    all_rewards.append(reward)\n",
    "    \n",
    "    if reward > bestreward:\n",
    "        bestreward = reward\n",
    "        bestparams = parameters\n",
    "        # considered solved if the agent lasts 200 timesteps\n",
    "        if reward == max_timesteps:\n",
    "            print(f'{i} episodes required to reach a reward of {max_timesteps}')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9.0,\n",
       " 8.0,\n",
       " 118.0,\n",
       " 10.0,\n",
       " 141.0,\n",
       " 11.0,\n",
       " 8.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 36.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 103.0,\n",
       " 58.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 77.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 88.0,\n",
       " 10.0,\n",
       " 8.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 80.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 167.0,\n",
       " 161.0,\n",
       " 65.0,\n",
       " 8.0,\n",
       " 10.0,\n",
       " 188.0,\n",
       " 500.0]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(all_rewards)\n",
    "plt.axvline(max_timesteps, color='r', linestyle='dashed', linewidth=1)\n",
    "plt.show()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0900372 , -0.49474541,  0.89044071,  0.72721733])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestreward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yop\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_episode(env, bestparams, max_timesteps, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "obs = env.reset()\n",
    "done = False\n",
    "i = 0\n",
    "total_rewards = []\n",
    "while not done:\n",
    "    i += 1\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    total_rewards.append(rewards)\n",
    "    time.sleep(0.08)\n",
    "    env.render()\n",
    "print(f'{i} episodes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Policy-Based Approach using non-Deep methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_training_data(env):\n",
    "    \n",
    "    \"\"\"build a policy space, linking of states to actions and associated rewards\"\"\"\n",
    "    number_of_games = 200\n",
    "    last_moves = 25\n",
    "    observations = []\n",
    "    actions = []\n",
    "\n",
    "    for i in range(number_of_games):\n",
    "        game_obs = []\n",
    "        game_acts = []\n",
    "        obs = env.reset()\n",
    "\n",
    "        for j in range(1000):\n",
    "            action = env.action_space.sample()\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            game_obs.append(obs)\n",
    "            game_acts.append(action)\n",
    "\n",
    "            if done:\n",
    "                observations += game_obs[:-(last_moves+1)]\n",
    "                actions += game_acts[1:-last_moves]\n",
    "                break\n",
    "\n",
    "    observations = np.array(observations)\n",
    "    actions = np.array(actions)\n",
    "\n",
    "    return observations, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_rl(env, m):\n",
    "    \"\"\"now play the game with a trained rl agent\"\"\"\n",
    "    #setup the game\n",
    "    obs = env.reset()\n",
    "\n",
    "    for i in range(1000):\n",
    "        #start to play the game\n",
    "        #model, tell me what to do next please\n",
    "        obs = obs.reshape(-1,4)\n",
    "        action = int(m.predict(obs))\n",
    "\n",
    "        #take an according step\n",
    "        obs,reward,done,_ = env.step(action)\n",
    "        #visusalise my results\n",
    "        env.render()\n",
    "        #print(obs, reward)\n",
    "        time.sleep(0.1)\n",
    "        #find out if i died\n",
    "        if done:\n",
    "            print(f'iterations survived {i}')\n",
    "            env.close()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations survived 157\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "#collect training data\n",
    "X,y = collect_training_data(env)\n",
    "#instantiate our agent, and fit on the data\n",
    "m = RandomForestClassifier()\n",
    "m.fit(X,y)\n",
    "#now play the game\n",
    "smart_rl(env,m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN\n",
    "* Can you implement a DQN with stable baselines?\n",
    "* Have a go wiht the [installation instructions](https://stable-baselines.readthedocs.io/en/master/guide/install.html) but please **use a virtualenv and check pre-installation requirements!!** (big installation issues can occur, especially with tensorflow  -consider using **docker** for the installation, and **pytorch** for the Deep networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stable_baselines'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-850d2735f7ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_env\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDummyVecEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicies\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMlpPolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDQN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'stable_baselines'"
     ]
    }
   ],
   "source": [
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines.deepq.policies import MlpPolicy\n",
    "from stable_baselines import DQN\n",
    "\n",
    "\n",
    "env = DummyVecEnv([lambda: gym.make('CartPole-v1')])\n",
    "model = DQN(MlpPolicy, env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "model.save(\"deepq_cartpole\")\n",
    "del model # remove to demonstrate saving and loading\n",
    "model = DQN.load(\"deepq_cartpole\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
