{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fundamental Difference between ML and DL\n",
    "\n",
    "- In \"simple\" ML (e.g. LinReg, LogReg, Trees, Bayesian Models), feature engineering is everything!\n",
    "    - hyperparameter optimization is *somewhat* useful.\n",
    "\n",
    "\n",
    "- In \"deep\" learning, hyperparameter optimization is everything. The onus of deciding the architecture / structure of the network is up to you!!\n",
    "    - a lot of the FE is being done for you already! This is done in the hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the hyperparameters in Neural Networks?\n",
    "    - 'Neural networks are not \"off-the-shelf\" algorithms in the way that random forest or logistic regression are. Even for simple, feed-forward networks, the onus is largely on the user to make numerous decisions about how the network is configured, connected, initialized and optimized.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Number of layers:\n",
    "    - the more layers, the more \"hidden features\" the model learns. Too many -> tends to overfit.\n",
    "    - too many layers --> `Vanishing Gradient Problem`: if the network is too \"deep\", then backprop starts to \"fizzle out\". Diminishing returns.\n",
    "- Number of epochs:\n",
    "    - the number of iterations in backpropagation: the more epochs, the more the model trains.\n",
    "- Activation Functions:\n",
    "    - Step\n",
    "        - primitive, not used in practice.\n",
    "    - Sigmoid\n",
    "        - gives probabilities\n",
    "        - IF YOU ARE DOING BINARY CLASSIFICATION, then the LAST LAYER MUST BE A SIGMOID. \n",
    "        - a single neuron in the last layer!\n",
    "    - ReLu \n",
    "        - normally used in hidden layers only\n",
    "        - trains super fast; the gradient / deriv is super easy to calculate\n",
    "        - us it for hidden layers!\n",
    "        - other variants: Leaky ReLU / ELU\n",
    "    - Linear\n",
    "        - no activation!\n",
    "        - turns your NN into a regression problem.\n",
    "    - Softmax:\n",
    "        - extension of logistic / sigmoid function for MULTIPLE CLASSES.\n",
    "        - this mathematical function ensures that all the probabilities add up to 1.\n",
    "        - IF YOU ARE DOING MULTICLASS CLASSIFICATION, then the LAST LAYER MUST USE A SOFTMAX FUNCTION.\n",
    "- Batch Size:\n",
    "    - Batch == \"sub\"-epoch \n",
    "    - larger batch size: faster training \n",
    "    - smaller batch size: lighter load on memory \n",
    "- Number of neurons in each layer:\n",
    "    - same as before. more neurons == more learning. too many neurons == maybe overfitting.\n",
    "- Optimizers:\n",
    "    - different variants of backprop\n",
    "- Weight Initialization:\n",
    "    - use the default one.\n",
    "- Type of layers:\n",
    "    - Fully Connected == \"Dense\" Layers \n",
    "    - Convolutional Kernels \n",
    "    - Recurrent Neural Network layers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Best Practices\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to avoid overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reduce complexity of model\n",
    "    - make the model complex at first, then then reduce.\n",
    "    - complexity ~= number of neurons\n",
    "- Increase data size:\n",
    "    - Image / Data Augmentation:\n",
    "        - see: Keras Data Augmentation / Image Augmention \n",
    "        - random pixel shifting / contract / rotation\n",
    "- Regularization:\n",
    "    - DropOut\n",
    "        - randomly shuts off some percentage of neurons during training. Prevents overspecialization. \n",
    "    - BatchNormalization\n",
    "        - standard scaling between layers -- improves training speed.\n",
    "        - basically the same thing as standard scaling as part of feature engineering in simple ML.\n",
    "- Transfer Learning\n",
    "    - You'll see this later.\n",
    "        - We can take existing networks that have been pre-trained and re-fit them for our own purposes.\n",
    "        - Some popular network: VGG-16 / VGG-19 (16-layers and 19-layers, respectively)\n",
    "        - ResNet50, which has 50 layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to save a model in Keras:**\n",
    "- there is a `m.save('model.h5')`\n",
    "- https://www.tensorflow.org/guide/keras/save_and_serialize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Reading:\n",
    "   - What should I do when my neural network doesn't learn? Best practices for when you are stuck: \n",
    "https://stats.stackexchange.com/questions/352036/what-should-i-do-when-my-neural-network-doesnt-learn\n",
    "        - The top answers in this post are extremely informative / give you more good \"rules of thumb\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
