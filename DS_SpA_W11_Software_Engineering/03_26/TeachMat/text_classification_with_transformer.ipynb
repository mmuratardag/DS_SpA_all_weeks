{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPX6cX0yd_e7"
   },
   "source": [
    "# Text classification with Transformer\n",
    "\n",
    "* Taken from Apoorv Nandan's work on the awesome [Super Dooper NLP repo](https://notebooks.quantumstat.com/#/)\n",
    "* Transformers are parallelisable networks - LSTMs are sequential\n",
    "* Transformers are faster and bigger than LSTMS (billions of parameters) - they're v expensive to train - (millions of dollars)\n",
    "* They are better than the competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory\n",
    "[The paper](https://arxiv.org/pdf/1706.03762.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Transformer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cool bit is Attention! Which Ill explain briefly\n",
    "* Check out [jalammer's blog](https://jalammar.github.io/illustrated-transformer/) for more info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZ0-1qVed_fA"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jV_GtviXd_fA"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "if not tf.__version__ == '2.4.1':\n",
    "    raise Exception('you might have to upgrade your tensorflow version!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAlWkdX-d_fB"
   },
   "source": [
    "## Implement a Transformer block as a layer\n",
    "* We use layer normalization not batch normalization here\n",
    "* In batch normalization, input values of the same neuron for all the data in the mini-batch are normalized. Whereas in layer normalization, input values for all neurons in the same layer are normalized for each data sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](normalization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[source](https://machinelearningknowledge.ai/keras-normalization-layers-explained-for-beginners-batch-normalization-vs-layer-normalization/#:~:text=Batch%20Normalization%20vs%20Layer%20Normalization,-Before%20wrapping%20up&text=In%20batch%20normalization%2C%20input%20values,normalized%20for%20each%20data%20sample.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "pMw9k3dFd_fB"
   },
   "outputs": [],
   "source": [
    "#override a Keras layer to provide Transformer functionality\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        # multi-head attention \n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        # passed into a feed forward network - one hidden layer, one output layer which generates the embeddings\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        #pass input through attention\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        #then to a ffn\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-7Qfio6d_fC"
   },
   "source": [
    "## Implement embedding layer\n",
    "\n",
    "Two seperate embedding layers, one for tokens, one for token index (positions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "zMcxfl2Wd_fC"
   },
   "outputs": [],
   "source": [
    "#Override a Keras layer to provide Embedding functionality\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        # word embedding input\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        # positional embedding input\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        # creates a tensor which has the shape of the input tensor - like np.ndarray\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        # creates a 1-d array, with start=start, limit=stop, delta=step \n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        # passes this through embedding layer\n",
    "        positions = self.pos_emb(positions)\n",
    "        # passes x through embedding layer\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iUtxJQ_ad_fC"
   },
   "source": [
    "## Download and prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/Users/tom_g/opt/anaconda3/envs/dl/lib/python3.6/site-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/Users/tom_g/opt/anaconda3/envs/dl/lib/python3.6/site-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "NUM_WORDS=20000\n",
    "INDEX_FROM=3 \n",
    "(X_train,y_train),(X_test_x,y_test) = keras.datasets.imdb.load_data(num_words=NUM_WORDS, index_from=INDEX_FROM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,y_train),(X_test,y_test) = keras.datasets.imdb.load_data(num_words=NUM_WORDS, index_from=INDEX_FROM)\n",
    "\n",
    "NUM_WORDS=20000\n",
    "INDEX_FROM=3   # word index offset - the first 4 chars are reserved, see below\n",
    "\n",
    "#get text equivalent of tokens\n",
    "word_to_id = keras.datasets.imdb.get_word_index()\n",
    "word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "word_to_id[\"<UNUSED>\"] = 3\n",
    "\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_sentence(x):\n",
    "    \"\"\"Print string equivalent of the tokens\"\"\"\n",
    "    print(' '.join([id_to_word[id] for id in x if id >0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.7598e+04, 5.3690e+03, 1.4070e+03, 5.1700e+02, 9.4000e+01,\n",
       "        7.0000e+00, 5.0000e+00, 2.0000e+00, 0.0000e+00, 1.0000e+00]),\n",
       " array([  11. ,  259.3,  507.6,  755.9, 1004.2, 1252.5, 1500.8, 1749.1,\n",
       "        1997.4, 2245.7, 2494. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAATFElEQVR4nO3df4xdZ33n8fdn7SaqKFEcMmu5dro2rKkU0K4JoxBpAWWbJXHCqg6rFbX/aFwaYRCJVNRdbZ3lj0R0I4VuKVIkNpVZLJwVxM02RLGKaTAW22ilDXgCrmMHgifGUcZy7ClmSXep0iZ894/7DHswM+OZudcznpn3S7q6537Pc855njk3/uT8mDOpKiRJy9s/WugOSJIWnmEgSTIMJEmGgSQJw0CSBKxc6A7M1dVXX13r169f6G5I0qLyzDPP/E1VDZ1fX7RhsH79ekZGRha6G5K0qCR5cbK6p4kkSYaBJMkwkCRhGEiSMAwkSRgGkiQMA0kSMwiDJLuTnE1ytFP7sySH2+tkksOtvj7J33Xm/WlnmXcmeTbJaJIHk6TVr0pyIMnx9r7qIoxTkjSNmRwZfAHY3C1U1W9V1aaq2gQ8Bny5M/uFiXlV9dFO/SHgw8DG9ppY507gYFVtBA62z5KkeXTB30CuqqeSrJ9sXvu/+w8CvzHdOpKsAa6oqqfb54eB24GvAluAG1vTPcD/AP5gJp2fq/U7v3IxVz+lkw+8f0G2K0kX0u81g/cAZ6rqeKe2Icl3kvxVkve02lpgrNNmrNUAVlfV6Tb9MrB6qo0l2ZFkJMnI+Ph4n12XJE3oNwy2AY90Pp8Gfq2q3gH8PvClJFfMdGXV+xucU/4dzqraVVXDVTU8NPQLz1mSJM3RnB9Ul2Ql8G+Ad07UqupV4NU2/UySF4C3AqeAdZ3F17UawJkka6rqdDuddHaufZIkzU0/Rwb/CvheVf3s9E+SoSQr2vSb6V0oPtFOA72S5IZ2neEO4Im22D5ge5ve3qlLkubJTG4tfQT4X8CvJxlLcmebtZWfP0UE8F7gSLvV9M+Bj1bVuTbvY8B/BUaBF+hdPAZ4AHhfkuP0AuaBuQ9HkjQXM7mbaNsU9d+ZpPYYvVtNJ2s/Arx9kvoPgZsu1A9J0sXjbyBLkgwDSZJhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCQxgzBIsjvJ2SRHO7X7kpxKcri9buvMuyfJaJLnk9zSqW9utdEkOzv1DUm+2ep/luSyQQ5QknRhMzky+AKweZL6Z6pqU3vtB0hyLbAVeFtb5r8kWZFkBfBZ4FbgWmBbawvwqbaufwr8CLiznwFJkmbvgmFQVU8B52a4vi3A3qp6tap+AIwC17fXaFWdqKq/B/YCW5IE+A3gz9vye4DbZzcESVK/+rlmcHeSI+000qpWWwu81Gkz1mpT1d8E/O+qeu28+qSS7EgykmRkfHy8j65LkrrmGgYPAW8BNgGngU8PqkPTqapdVTVcVcNDQ0PzsUlJWhZWzmWhqjozMZ3kc8BftI+ngGs6Tde1GlPUfwhcmWRlOzrotpckzZM5HRkkWdP5+AFg4k6jfcDWJJcn2QBsBL4FHAI2tjuHLqN3kXlfVRXwDeDftuW3A0/MpU+SpLm74JFBkkeAG4Grk4wB9wI3JtkEFHAS+AhAVR1L8ijwHPAacFdVvd7WczfwJLAC2F1Vx9om/gDYm+Q/Ad8BPj+owUmSZuaCYVBV2yYpT/kPdlXdD9w/SX0/sH+S+gl6dxtJkhaIv4EsSTIMJEmGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJYgZhkGR3krNJjnZq/znJ95IcSfJ4kitbfX2Sv0tyuL3+tLPMO5M8m2Q0yYNJ0upXJTmQ5Hh7X3URxilJmsZMjgy+AGw+r3YAeHtV/TPg+8A9nXkvVNWm9vpop/4Q8GFgY3tNrHMncLCqNgIH22dJ0jy6YBhU1VPAufNqX6uq19rHp4F1060jyRrgiqp6uqoKeBi4vc3eAuxp03s6dUnSPBnENYPfBb7a+bwhyXeS/FWS97TaWmCs02as1QBWV9XpNv0ysHqqDSXZkWQkycj4+PgAui5Jgj7DIMkngNeAL7bSaeDXquodwO8DX0pyxUzX144aapr5u6pquKqGh4aG+ui5JKlr5VwXTPI7wL8Gbmr/iFNVrwKvtulnkrwAvBU4xc+fSlrXagBnkqypqtPtdNLZufZJkjQ3czoySLIZ+A/Ab1bVTzr1oSQr2vSb6V0oPtFOA72S5IZ2F9EdwBNtsX3A9ja9vVOXJM2TCx4ZJHkEuBG4OskYcC+9u4cuBw60O0SfbncOvRf4ZJJ/AH4KfLSqJi4+f4zenUm/TO8aw8R1hgeAR5PcCbwIfHAgI5MkzdgFw6Cqtk1S/vwUbR8DHpti3gjw9knqPwRuulA/JEkXj7+BLEkyDCRJhoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CSxAzDIMnuJGeTHO3UrkpyIMnx9r6q1ZPkwSSjSY4kua6zzPbW/niS7Z36O5M825Z5MEkGOUhJ0vRmemTwBWDzebWdwMGq2ggcbJ8BbgU2ttcO4CHohQdwL/Au4Hrg3okAaW0+3Fnu/G1Jki6iGYVBVT0FnDuvvAXY06b3ALd36g9Xz9PAlUnWALcAB6rqXFX9CDgAbG7zrqiqp6uqgIc765IkzYN+rhmsrqrTbfplYHWbXgu81Gk31mrT1ccmqf+CJDuSjCQZGR8f76PrkqSugVxAbv9HX4NY1wW2s6uqhqtqeGho6GJvTpKWjX7C4Ew7xUN7P9vqp4BrOu3Wtdp09XWT1CVJ86SfMNgHTNwRtB14olO/o91VdAPw43Y66Ung5iSr2oXjm4En27xXktzQ7iK6o7MuSdI8WDmTRkkeAW4Erk4yRu+uoAeAR5PcCbwIfLA13w/cBowCPwE+BFBV55L8IXCotftkVU1clP4YvTuWfhn4antJkubJjMKgqrZNMeumSdoWcNcU69kN7J6kPgK8fSZ9kSQNnr+BLEkyDCRJhoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CSRB9hkOTXkxzuvF5J8vEk9yU51anf1lnmniSjSZ5PckunvrnVRpPs7HdQkqTZWTnXBavqeWATQJIVwCngceBDwGeq6o+77ZNcC2wF3gb8KvD1JG9tsz8LvA8YAw4l2VdVz821b5Kk2ZlzGJznJuCFqnoxyVRttgB7q+pV4AdJRoHr27zRqjoBkGRva2sYSNI8GdQ1g63AI53Pdyc5kmR3klWtthZ4qdNmrNWmqkuS5knfYZDkMuA3gf/eSg8Bb6F3Cuk08Ol+t9HZ1o4kI0lGxsfHB7VaSVr2BnFkcCvw7ao6A1BVZ6rq9ar6KfA5/v+poFPANZ3l1rXaVPVfUFW7qmq4qoaHhoYG0HVJEgwmDLbROUWUZE1n3geAo216H7A1yeVJNgAbgW8Bh4CNSTa0o4ytra0kaZ70dQE5yRvo3QX0kU75j5JsAgo4OTGvqo4leZTeheHXgLuq6vW2nruBJ4EVwO6qOtZPvyRJs9NXGFTV/wXedF7tt6dpfz9w/yT1/cD+fvoiSZo7fwNZkmQYSJIMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJWLnQHVhO1u/8yoJt++QD71+wbUu69PV9ZJDkZJJnkxxOMtJqVyU5kOR4e1/V6knyYJLRJEeSXNdZz/bW/niS7f32S5I0c4M6TfQvq2pTVQ23zzuBg1W1ETjYPgPcCmxsrx3AQ9ALD+Be4F3A9cC9EwEiSbr4LtY1gy3Anja9B7i9U3+4ep4GrkyyBrgFOFBV56rqR8ABYPNF6psk6TyDCIMCvpbkmSQ7Wm11VZ1u0y8Dq9v0WuClzrJjrTZV/eck2ZFkJMnI+Pj4ALouSYLBXEB+d1WdSvKPgQNJvtedWVWVpAawHapqF7ALYHh4eCDrlCQN4Migqk6197PA4/TO+Z9pp39o72db81PANZ3F17XaVHVJ0jzoKwySvCHJGyemgZuBo8A+YOKOoO3AE216H3BHu6voBuDH7XTSk8DNSVa1C8c3t5okaR70e5poNfB4kol1famq/jLJIeDRJHcCLwIfbO33A7cBo8BPgA8BVNW5JH8IHGrtPllV5/rsmyRphvoKg6o6AfzzSeo/BG6apF7AXVOsazewu5/+SJLmxsdRSJIMA0mSYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAk0UcYJLkmyTeSPJfkWJLfa/X7kpxKcri9bussc0+S0STPJ7mlU9/caqNJdvY3JEnSbK3sY9nXgH9XVd9O8kbgmSQH2rzPVNUfdxsnuRbYCrwN+FXg60ne2mZ/FngfMAYcSrKvqp7ro2+SpFmYcxhU1WngdJv+2yTfBdZOs8gWYG9VvQr8IMkocH2bN1pVJwCS7G1tDQNJmicDuWaQZD3wDuCbrXR3kiNJdidZ1WprgZc6i4212lT1ybazI8lIkpHx8fFBdF2SxADCIMmvAI8BH6+qV4CHgLcAm+gdOXy6321MqKpdVTVcVcNDQ0ODWq0kLXv9XDMgyS/RC4IvVtWXAarqTGf+54C/aB9PAdd0Fl/XakxTlyTNg37uJgrweeC7VfUnnfqaTrMPAEfb9D5ga5LLk2wANgLfAg4BG5NsSHIZvYvM++baL0nS7PVzZPAvgN8Gnk1yuNX+I7AtySaggJPARwCq6liSR+ldGH4NuKuqXgdIcjfwJLAC2F1Vx/rolyRplvq5m+h/Aplk1v5plrkfuH+S+v7plpMkXVz+BrIkyTCQJBkGkiQMA0kShoEkCcNAkoRhIEmiz8dRaPFYv/MrC7Ldkw+8f0G2K2l2PDKQJBkGkiTDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJH0ehi2yhHoMBPgpDmg2PDCRJhoEk6RIKgySbkzyfZDTJzoXujyQtJ5dEGCRZAXwWuBW4FtiW5NqF7ZUkLR+XygXk64HRqjoBkGQvsAV4bkF7pUXNv+EgzdylEgZrgZc6n8eAd53fKMkOYEf7+H+SPD+HbV0N/M0cllvMHPM8yqcWYqs/475eHvoZ8z+ZrHiphMGMVNUuYFc/60gyUlXDA+rSouCYl4/lOG7HPBiXxDUD4BRwTefzulaTJM2DSyUMDgEbk2xIchmwFdi3wH2SpGXjkjhNVFWvJbkbeBJYAeyuqmMXaXN9nWZapBzz8rEcx+2YByBVNeh1SpIWmUvlNJEkaQEZBpKk5RMGS/lxF0lOJnk2yeEkI612VZIDSY6391WtniQPtp/DkSTXLWzvZy7J7iRnkxzt1GY9ziTbW/vjSbYvxFhmaoox35fkVNvfh5Pc1pl3Txvz80lu6dQXzfc/yTVJvpHkuSTHkvxeqy/ZfT3NmOdvX1fVkn/Ruyj9AvBm4DLgr4FrF7pfAxzfSeDq82p/BOxs0zuBT7Xp24CvAgFuAL650P2fxTjfC1wHHJ3rOIGrgBPtfVWbXrXQY5vlmO8D/v0kba9t3+3LgQ3tO79isX3/gTXAdW36jcD329iW7L6eZszztq+Xy5HBzx53UVV/D0w87mIp2wLsadN7gNs79Yer52ngyiRrFqB/s1ZVTwHnzivPdpy3AAeq6lxV/Qg4AGy+6J2foynGPJUtwN6qerWqfgCM0vvuL6rvf1Wdrqpvt+m/Bb5L7ykFS3ZfTzPmqQx8Xy+XMJjscRfT/aAXmwK+luSZ9sgOgNVVdbpNvwysbtNL7Wcx23EulfHf3U6J7J44XcISHHOS9cA7gG+yTPb1eWOGedrXyyUMlrp3V9V19J76eleS93ZnVu+4csnfQ7xcxgk8BLwF2AScBj69oL25SJL8CvAY8PGqeqU7b6nu60nGPG/7ermEwZJ+3EVVnWrvZ4HH6R0qnpk4/dPez7bmS+1nMdtxLvrxV9WZqnq9qn4KfI7e/oYlNOYkv0TvH8UvVtWXW3lJ7+vJxjyf+3q5hMGSfdxFkjckeePENHAzcJTe+CbuntgOPNGm9wF3tDswbgB+3Dn0XoxmO84ngZuTrGqH3De32qJx3jWeD9Db39Ab89YklyfZAGwEvsUi+/4nCfB54LtV9SedWUt2X0815nnd1wt9FX2+XvTuOPg+vSvtn1jo/gxwXG+md8fAXwPHJsYGvAk4CBwHvg5c1eqh94eEXgCeBYYXegyzGOsj9A6V/4HeudA75zJO4HfpXXAbBT600OOaw5j/WxvTkfYf+ppO+0+0MT8P3NqpL5rvP/BueqeAjgCH2+u2pbyvpxnzvO1rH0chSVo2p4kkSdMwDCRJhoEkyTCQJGEYSJIwDCRJGAaSJOD/AfuxWOx1urUhAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(x) for x in X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "visualise_sentence(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "rW0ZsKV1d_fC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 Training sequences\n",
      "25000 Validation sequences\n"
     ]
    }
   ],
   "source": [
    "maxlen = 200  # Only consider the first 200 words of each movie review\n",
    "print(len(X_train), \"Training sequences\")\n",
    "print(len(X_test), \"Validation sequences\")\n",
    "X_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aIDNgsIVd_fD"
   },
   "source": [
    "## Create classifier model using transformer layer\n",
    "\n",
    "Transformer layer outputs one vector for each time step of our input sequence.\n",
    "Here, we take the mean across all time steps to classify text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "OL-s9gp7d_fD"
   },
   "outputs": [],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "vocab_size=20000\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "model.add(embedding_layer)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "model.add(transformer_block)\n",
    "model.add(layers.GlobalAveragePooling1D())\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Dense(20, activation=\"relu\"))\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Dense(2, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6V1MeFpud_fD"
   },
   "source": [
    "## Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Overfitting is a serious problem in NLP\n",
    "* The metric that models are scored on often is BLEU score, which a general of language tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "FgmJ6TJNd_fD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "625/625 [==============================] - 48s 76ms/step - loss: 0.5305 - accuracy: 0.7029 - val_loss: 0.3057 - val_accuracy: 0.8774\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 52s 84ms/step - loss: 0.1966 - accuracy: 0.9274 - val_loss: 0.3541 - val_accuracy: 0.8564\n"
     ]
    }
   ],
   "source": [
    "model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train, batch_size=32, epochs=2, validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on some unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 23s 29ms/step - loss: 0.3714 - accuracy: 0.8470\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.37143874168395996, 0.8469600081443787]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text is:\n",
      "\n",
      "<START> <UNK> of white and various shapes a beautiful nude random images that is what this little experimental short film is br br it's kind of interesting to think how in the early days of film such images could be transferred onto film but despite my love of a lot of surreal images and films and a fascination with the bizarre this film just didn't do it for me br br i'm not sorry i watched it but if there is any underlying meaning in it i don't get it visually it is not that outstanding in my humble opinion as an example of i suppose it would fit in quite well since it seems to reject any semblance of logic or reason though i would have preferred that it do it in a more visually interesting way br br but to each his own\n",
      "None\n",
      "\n",
      "The models evaluation of sentiment is: \n",
      "\n",
      "Positive: 0.71\n",
      "Negative: 0.29\n"
     ]
    }
   ],
   "source": [
    "random_text = X_test[np.random.randint(len(X_test))]\n",
    "result = model.predict(random_text).mean(axis=0)\n",
    "print('Original text is:\\n')\n",
    "print(visualise_sentence(random_text))\n",
    "print()\n",
    "print('The models evaluation of sentiment is: \\n')\n",
    "print('Positive: ' + str(round(result[1],2)))\n",
    "print('Negative: ' + str(round(result[0],2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS - Use [hugging face](https://huggingface.co/), #1 API for pre-trained Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS - Use Spacy who also have transformers in their python package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "text_classification_with_transformer",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
