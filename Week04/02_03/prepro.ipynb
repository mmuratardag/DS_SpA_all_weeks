{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 666\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from time import time\n",
    "\n",
    "import re, random\n",
    "\n",
    "from nltk import word_tokenize, sent_tokenize, pos_tag\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics</th>\n",
       "      <th>artist</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I know a girl who thinks it goes she'll make y...</td>\n",
       "      <td>flaming lips</td>\n",
       "      <td>indie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>All those bugs buzzin' round your head Well, t...</td>\n",
       "      <td>flaming lips</td>\n",
       "      <td>indie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Something in you, it jitters like a moth And I...</td>\n",
       "      <td>flaming lips</td>\n",
       "      <td>indie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Their wasn't any snow on Christmas eve and I k...</td>\n",
       "      <td>flaming lips</td>\n",
       "      <td>indie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You can walk among us, but you can't walk on b...</td>\n",
       "      <td>flaming lips</td>\n",
       "      <td>indie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              lyrics        artist  genre\n",
       "0  I know a girl who thinks it goes she'll make y...  flaming lips  indie\n",
       "1  All those bugs buzzin' round your head Well, t...  flaming lips  indie\n",
       "2  Something in you, it jitters like a moth And I...  flaming lips  indie\n",
       "3  Their wasn't any snow on Christmas eve and I k...  flaming lips  indie\n",
       "4  You can walk among us, but you can't walk on b...  flaming lips  indie"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_df = pd.read_csv('corpus_df.csv').drop(columns=['Unnamed: 0'])\n",
    "corpus_df.head()\n",
    "# corpus_df.shape # (1693, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_df['genre'].value_counts()\n",
    "# punk     1097\n",
    "# indie     596"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1269,), (424,), (1269,), (424,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(corpus_df['lyrics'], corpus_df['genre'], test_size=.25, random_state=seed, stratify=corpus_df['genre'])\n",
    "# train = pd.concat([X_train, y_train], axis=1)\n",
    "# test = pd.concat([X_test, y_test], axis=1)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['genre'].value_counts()\n",
    "# punk     822\n",
    "# indie    447\n",
    "\n",
    "# test['genre'].value_counts()\n",
    "# punk     275\n",
    "# indie    149"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect(vectoriser, X):\n",
    "    # Fit and transform\n",
    "    start = time()\n",
    "    print(f\"There are {vectoriser.fit_transform(X).shape[1]} columns.\\n\")\n",
    "    end = time()\n",
    "    print(f\"Took {round((end-start),2)} seconds.\\n\")\n",
    "    \n",
    "    # Inspect tokens\n",
    "    tokens = list(vectoriser.vocabulary_.keys())\n",
    "    tokens.sort()\n",
    "    print(f\"Example tokens: {tokens[:50]}\\n\")\n",
    "    \n",
    "    # Inspect ignored tokens\n",
    "    ignored = vectoriser.stop_words_\n",
    "    if len(ignored)==0:\n",
    "        print(\"No token is ignored.\")\n",
    "    elif len(ignored)>50:\n",
    "        print(f\"Example ignored tokens: {random.sample(ignored, 50)}\")\n",
    "    else:\n",
    "        print(f\"Example ignored tokens: {ignored}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1110 columns.\n",
      "\n",
      "Took 0.14 seconds.\n",
      "\n",
      "Example tokens: ['accept', 'act', 'acting', 'afraid', 'age', 'ago', 'ah', 'ahead', 'ain', 'air', 'alive', 'alright', 'america', 'american', 'angel', 'angels', 'anger', 'angry', 'animal', 'answer', 'answers', 'anybody', 'anymore', 'apart', 'aren', 'arm', 'arms', 'ask', 'asked', 'asking', 'ass', 'attention', 'authority', 'away', 'babe', 'baby', 'bad', 'ball', 'band', 'bar', 'battle', 'bear', 'beat', 'beautiful', 'beauty', 'bed', 'beer', 'began', 'begin', 'beginning']\n",
      "\n",
      "Example ignored tokens: ['dividethey', 'agei', 'autumn', 'trammels', 'slowin', 'torn', 'estael', 'sanity', 'wino', 'steve', 'smaller', 'remains', 'stuffi', 'sacredthey', 'skill', 'goclose', 'thatnext', 'fascism', 'desequilibrium', 'enormous', 'factories', 'carewe', 'discriminate', 'loosen', 'suicides', 'bleat', 'pipe', 'untruths', 'docking', 'absolutely', 'romancewarm', 'volition', 'grandmother', 'aux', 'celebrate', 'escapeour', 'futility', 'waylooking', 'handgimme', 'male', 'timeslumber', 'lightcan', 'swamp', 'slay', 'shapes', 'twinkies', 'debrisoh', 'notions', 'overload', 'reports']\n"
     ]
    }
   ],
   "source": [
    "vectoriser = TfidfVectorizer(token_pattern=r'[a-z]+', stop_words='english', min_df=10, max_df=.97)\n",
    "inspect(vectoriser, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1269x1110 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 41098 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = vectoriser.fit_transform(X_train)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accept</th>\n",
       "      <th>act</th>\n",
       "      <th>acting</th>\n",
       "      <th>afraid</th>\n",
       "      <th>age</th>\n",
       "      <th>ago</th>\n",
       "      <th>ah</th>\n",
       "      <th>ahead</th>\n",
       "      <th>ain</th>\n",
       "      <th>air</th>\n",
       "      <th>...</th>\n",
       "      <th>yea</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yes</th>\n",
       "      <th>york</th>\n",
       "      <th>youi</th>\n",
       "      <th>young</th>\n",
       "      <th>youth</th>\n",
       "      <th>yr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058555</td>\n",
       "      <td>0.038367</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.230098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.118609</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1265</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.20991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1268</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1269 rows × 1110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      accept  act  acting  afraid       age  ago   ah  ahead       ain  \\\n",
       "0        0.0  0.0     0.0     0.0  0.000000  0.0  0.0    0.0  0.000000   \n",
       "1        0.0  0.0     0.0     0.0  0.000000  0.0  0.0    0.0  0.058555   \n",
       "2        0.0  0.0     0.0     0.0  0.000000  0.0  0.0    0.0  0.000000   \n",
       "3        0.0  0.0     0.0     0.0  0.000000  0.0  0.0    0.0  0.000000   \n",
       "4        0.0  0.0     0.0     0.0  0.118609  0.0  0.0    0.0  0.000000   \n",
       "...      ...  ...     ...     ...       ...  ...  ...    ...       ...   \n",
       "1264     0.0  0.0     0.0     0.0  0.000000  0.0  0.0    0.0  0.000000   \n",
       "1265     0.0  0.0     0.0     0.0  0.000000  0.0  0.0    0.0  0.000000   \n",
       "1266     0.0  0.0     0.0     0.0  0.000000  0.0  0.0    0.0  0.000000   \n",
       "1267     0.0  0.0     0.0     0.0  0.000000  0.0  0.0    0.0  0.000000   \n",
       "1268     0.0  0.0     0.0     0.0  0.000000  0.0  0.0    0.0  0.000000   \n",
       "\n",
       "           air  ...  yea      yeah  year    years  yes  york  youi  young  \\\n",
       "0     0.000000  ...  0.0  0.000000   0.0  0.00000  0.0   0.0   0.0    0.0   \n",
       "1     0.038367  ...  0.0  0.230098   0.0  0.00000  0.0   0.0   0.0    0.0   \n",
       "2     0.000000  ...  0.0  0.000000   0.0  0.00000  0.0   0.0   0.0    0.0   \n",
       "3     0.000000  ...  0.0  0.000000   0.0  0.00000  0.0   0.0   0.0    0.0   \n",
       "4     0.000000  ...  0.0  0.000000   0.0  0.00000  0.0   0.0   0.0    0.0   \n",
       "...        ...  ...  ...       ...   ...      ...  ...   ...   ...    ...   \n",
       "1264  0.000000  ...  0.0  0.000000   0.0  0.00000  0.0   0.0   0.0    0.0   \n",
       "1265  0.000000  ...  0.0  0.000000   0.0  0.00000  0.0   0.0   0.0    0.0   \n",
       "1266  0.000000  ...  0.0  0.000000   0.0  0.20991  0.0   0.0   0.0    0.0   \n",
       "1267  0.000000  ...  0.0  0.000000   0.0  0.00000  0.0   0.0   0.0    0.0   \n",
       "1268  0.000000  ...  0.0  0.000000   0.0  0.00000  0.0   0.0   0.0    0.0   \n",
       "\n",
       "      youth   yr  \n",
       "0       0.0  0.0  \n",
       "1       0.0  0.0  \n",
       "2       0.0  0.0  \n",
       "3       0.0  0.0  \n",
       "4       0.0  0.0  \n",
       "...     ...  ...  \n",
       "1264    0.0  0.0  \n",
       "1265    0.0  0.0  \n",
       "1266    0.0  0.0  \n",
       "1267    0.0  0.0  \n",
       "1268    0.0  0.0  \n",
       "\n",
       "[1269 rows x 1110 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X.todense(), columns=vectoriser.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_text(text):\n",
    "#     # 1. Tokenise to alphabetic tokens\n",
    "#     tokeniser = RegexpTokenizer(r'[A-Za-z]+')\n",
    "#     tokens = tokeniser.tokenize(text)\n",
    "    \n",
    "#     # 2. POS tagging\n",
    "#     pos_map = {'J': 'a', 'N': 'n', 'R': 'r', 'V': 'v'}\n",
    "#     pos_tags = pos_tag(tokens)\n",
    "    \n",
    "#     # 3. Lowercase and lemmatise \n",
    "#     lemmatiser = WordNetLemmatizer()\n",
    "#     tokens = [lemmatiser.lemmatize(t.lower(), pos=pos_map.get(p[0], 'v')) for t, p in pos_tags]\n",
    "\n",
    "#     return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectoriser = TfidfVectorizer(analyzer=preprocess_text, min_df=10, max_df=.97)\n",
    "# X = vectoriser.fit_transform(X_train)\n",
    "# X"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
